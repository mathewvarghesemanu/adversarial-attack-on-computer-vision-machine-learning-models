# Adversarial Machine Learning-Attacking the MNIST Dataset
Adversarial machine learning is a rapidly growing field that has the potential to expose vulnerabilities in machine learning models and inform more robust model development. One popular dataset for studying adversarial attacks is the MNIST dataset, which contains handwritten digits. However, the field has since evolved and the MNIST dataset is no longer the only testbed used to develop and evaluate adversarial attacks.
<br>
This repository is dedicated to hosting adversarial attacks and their results for three popular techniques - Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and DeepFool. The repository aims to provide a comprehensive collection of adversarial attacks and results on a variety of datasets, architectures, and hyperparameters. The code and documentation provided here can be used as a reference for researchers, developers, and enthusiasts interested in exploring adversarial machine learning.

# Technologies used
<li> Python </li>
<li> Tensorflow </li>
<li> Pytorch </li>

# collaborators
<li>Mathew Varghese</li>
<li>Rishabh Gupta</li>